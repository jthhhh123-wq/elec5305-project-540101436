# ğŸ™ï¸ Noise-Robust Keyword Spotting with ConvMixer

This repository contains the code for an ELEC5305 project on **noise-robust keyword spotting (KWS)** using a lightweight **ConvMixer** architecture.  
The system recognises short speech commands from the **Google Speech Commands v0.02** dataset under multiple noise conditions.

The project progressively builds from:

- a **clean baseline** model,
- a **noise-augmented AWGN** model,
- an extended **AWGN_v2** model with stronger noise,
- a **curriculum learning** strategy over SNR,
- and an **SE+Reverb** variant using speech enhancement and reverberation.

All experiments share the same backbone architecture to enable fair comparison.
---

## ğŸ“‚ Folder Structure

```bash
noise_robust_kws_convmixer/
â”‚
â”œâ”€â”€ baseline/                # Baseline training & evaluation pipeline
â”œâ”€â”€ experiments/             # Additional training strategies
â”‚   â”œâ”€â”€ awgn/                # AWGN v1 and v2
â”‚   â”œâ”€â”€ curriculum/          # Curriculum learning
â”‚   â””â”€â”€ se_reverb/           # Speech enhancement + reverb
â”‚
â”œâ”€â”€ data/                    # Place Speech Commands v0.02 here
â”‚   â””â”€â”€ readme.md            # Dataset download instructions
â”‚
â”œâ”€â”€ figures/                 # Project figures (mel-spectrogram, etc.)
â”‚
â”œâ”€â”€ runs/                    # All experiment outputs
â”‚   â”œâ”€â”€ baseline_gpu_25ep/
â”‚   â”œâ”€â”€ awgn/
â”‚   â”œâ”€â”€ awgn_v2/
â”‚   â”œâ”€â”€ curriculum/
â”‚   â”œâ”€â”€ se_reverb/
â”‚   â”œâ”€â”€ acc_snr.csv          # Combined results of all experiments
â”‚   â””â”€â”€ acc_snr.png          # Accuracyâ€“SNR comparison plot
â”‚
â”œâ”€â”€ README.md                # Main project documentation
â””â”€â”€ plot_melspec.py          # Script for generating mel-spectrogram

```
baseline/: code and config for the clean ConvMixer baseline.

experiments/awgn/: AWGN, AWGN_v2, curriculum and SE+Reverb experiments (all built on the same structure).

runs/: all trained checkpoints and evaluation outputs (CSV + plots) are written here.

data/SpeechCommands/: location of the Google Speech Commands dataset.

## âš™ï¸ 1. Environment and Data Setup
### 1.1 Create and activate conda environment

In any terminal:
```bash
conda create -n kws python=3.10
conda activate kws
```

### 1.2 Install dependencies
From the project root:
```bash
pip install torch torchaudio matplotlib pyyaml
```

### 1.3 Hardware environment
The experiments were run on:
```bash
GPU: NVIDIA RTX 4060 (8GB)
Framework: PyTorch (CUDA enabled)
```
(If CUDA is unavailable, the scripts will automatically fall back to CPU, but training will be significantly slower.)

### 1.4 Prepare dataset
Download Google Speech Commands v0.02 and place it under:
```bash
project_root/data/SpeechCommands/
```
he scripts will automatically handle train/validation/test splits using torchaudioâ€™s interface.

### 1.5 Visualizing the Mel-Spectrogram (Figure 1)

To better understand the input features used by the ConvMixer keyword-spotting model, this project includes a visualization of a 64-bin Mel-spectrogram generated from a sample command (e.g., â€œyesâ€). The Mel-spectrogram provides a compact and perceptually meaningful timeâ€“frequency representation that serves as the modelâ€™s input during training and evaluation.

This visualization helps illustrate:

Low-frequency bands that carry most speech energy

High-frequency components related to formant transitions and consonants

The overall timeâ€“frequency structure the model learns to classify

Why Mel-spectrograms are robust and widely used in speech recognition pipelines

â–¶ï¸ How to Generate the Mel-Spectrogram Figure

Run the provided plotting script from the project root directory:
```bash
conda activate kws
python plot_melspec.py
```
This script loads an example Speech Commands waveform, computes the Mel-spectrogram using the same parameters as the training pipeline (1024 FFT, 320 hop length, 64 Mel bins), converts it to log-amplitude scale, and visualizes it.

ğŸ“ Where the Figure Is Saved

The output image will be saved automatically as:

```bash
figures/melspec_yes.png
```
![Example Mel-Spectrogram](figures/melspec_yes.png)

## ğŸš€ 2. Run Baseline
ğŸ“ Terminal location: open a terminal in the baseline/ folder.

### 2.1 Train the baseline model
```bash
conda activate kws
python -m src.train --data_dir ../data --config ./configs/baseline.yaml --ckpt_dir ../runs/baseline_gpu_25ep
```

### 2.2 Evaluate robustness under noise
```bash
python -m src.eval_noise_sweep --data_dir ../data --config ./configs/baseline.yaml --ckpt ../runs/baseline_gpu_25ep/baseline_best.pt
```
Results will be automatically saved to:
```bash
runs/acc_snr.csv
```
### 2.3 Plot baseline Accuracy vs SNR
From the project root:
```bash
conda activate kws
python experiments/plot_acc_snr.py --csv runs/acc_snr.csv --out runs/acc_snr.png --title "Baseline"
```
This generates:
```bash
runs/acc_snr.png
```
containing the baseline accuracyâ€“SNR curve


## ğŸ”Š 3. Run AWGN Experiment
This version adds Additive White Gaussian Noise (AWGN) during training to improve robustness.

### 3.1 Train the AWGN model
ğŸ“ Terminal location: open a terminal in experiments/awgn/.
```bash
conda activate kws
python -m src.train --data_dir ../../data --config ./configs/awgn_train.yaml --ckpt_dir ../../runs/awgn
```

### 3.2 Evaluate the model
```bash
python -m src.eval_noise_sweep --data_dir ../../data --config ./configs/awgn_train.yaml --ckpt ../../runs/awgn/awgn_best.pt
```
Results will append to:
```bash
runs/acc_snr.csv
```
### 3.3 Compare baseline vs AWGN
From the project root:
you can visualize the comparison:
```bash
python experiments/plot_acc_snr.py --csv runs/acc_snr.csv --out runs/acc_snr.png --title "Baseline vs AWGN"
```

## ğŸ§© 4. Run AWGN_v2 (Improved Version)

This version deepens the model and extends the noise range to better handle low-SNR conditions.

### 4.1 Train the improved model
ğŸ“ Terminal location: open a terminal in experiments/awgn/.
```bash
conda activate kws
python -m src.train --data_dir ../../data --config ./configs/awgn_train_v2.yaml --ckpt_dir ../../runs/awgn_v2
```
### 4.2 Evaluate the model
```bash
python -m src.eval_noise_sweep --data_dir ../../data --config ./configs/awgn_train_v2.yaml --ckpt ../../runs/awgn_v2/awgn_best.pt
```
Results will append to:
```bash
runs/acc_snr.csv
```

### 4.3 Plot Accuracy vs SNR
After running baseline + AWGN + AWGN_v2, from the project root:
```bash
python experiments/plot_acc_snr.py --csv runs/acc_snr.csv --out runs/acc_snr.png --title "Baseline vs AWGN vs AWGN_v2"
```
The plot and CSV are saved to:
```bash
runs/acc_snr.csv
runs/acc_snr.png
```

## ğŸ“˜ 5. Curriculum Learning Training
This experiment trains the model using multi-stage SNR curriculum learning, where training starts at high SNR (easy) and gradually moves to low SNR (hard).
All settings are defined in:
experiments/curriculum/configs/curriculum_train.yaml

###ğŸš€ 5.1 Training (Curriculum Learning)
ğŸ“ Enter the curriculum folderï¼šopen a terminal in experiments/curriculum/.

Run the curriculum-learning training:

```bash
```bash
conda activate kws
python -m src.train --data_dir ../../data --config ./configs/curriculum_train.yaml --ckpt_dir ../../runs/curriculum
```
```

This will:

Train sequentially on SNR stages (e.g., 20 â†’ 10 â†’ 0 â†’ â€“5 dB)

Save checkpoints to runs/curriculum/

Save the best model as

```bash
runs/curriculum/<tag>_best.pt
```

### ğŸ“Š 5.2 Noise Robustness Evaluation

After training, evaluate the model under different SNR values.

Run from the curriculum experiment folder:

```bash
python -m src.eval_noise_sweep --data_dir ../../data --config ./configs/curriculum_train.yaml --ckpt ../../runs/curriculum/curriculum_best.pt
```
This script will:

Test the trained model at SNR = 20, 10, 0, â€“5 dB

Append results to:

```bash
runs/acc_snr.csv
```

### ğŸ“ˆ 5.3 Plotting the Accuracyâ€“SNR Curve

From the project root:
Use the unified plotting script:

```bash
python experiments/plot_acc_snr.py --csv runs/acc_snr.csv --out runs/acc_snr.png --title "Baseline vs AWGN vs Curriculum"
```

This generates:

```bash
runs/acc_snr.png
```

The plot will contain all experiments whose results are recorded in the CSV, such as:

baseline

awgn_train

awgn_train_v2

curriculum_train

### âœ” Expected Result Trend

Curriculum learning typically provides:

Slightly worse performance at very high noise (â€“5 dB) compared to AWGN v2

Clear improvement at moderate/high SNR (10â€“20 dB)

Smooth and stable SNRâ€“Accuracy curve

## ğŸ“˜ 6. se_reverb Training

This experiment evaluates a speech enhancement + reverberation strategy.

### 6.1 Train the SE_Reverb model

ğŸ“ Terminal location: open a terminal in experiments/awgn/
From the `experiments/awgn` folder:

```bash
conda activate kws
python -m src.train --data_dir ../../data --config ./configs/se_reverb.yaml --ckpt_dir ../../runs/se_reverb
```

### 6.2 Evaluate noise robustness
```bash
python -m src.eval_noise_sweep --data_dir ../../data --config ./configs/se_reverb.yaml --ckpt ../../runs/se_reverb/se_reverb_best.pt
```

### 6.3 Plot final comparison including SE_Reverb
```bash
python experiments/plot_acc_snr.py --csv runs/acc_snr.csv --out runs/acc_snr.png --title "Baseline vs AWGN vs Curriculum vs se_reverb"
```

## ğŸ“Š Results Overview 
The overall results demonstrate that model robustness depends strongly on the chosen training strategy.
The clean baseline performs well in noise-free conditions but degrades quickly as SNR decreases.
Training with Gaussian noise (AWGN) provides moderate improvements, with AWGN_v2 showing slightly better low-SNR performance than AWGN_v1.
Curriculum learning consistently achieves the best accuracy across all noise levels, offering both high performance in clean conditions and strong resilience under severe noise.
In contrast, the SE_Reverb model performs worst across all SNRs due to enhancement artifacts and reverberation mismatch, confirming that inappropriate augmentation can harm robustness.
The combined accuracyâ€“SNR plot clearly highlights these differences and visually illustrates the benefit of structured training over naive augmentation.
You can download or view the raw results here:

ğŸ‘‰ [acc_snr.csv](runs/acc_snr.csv)

Below is the final accuracyâ€“SNR comparison plot including all experiments:
The final accuracyâ€“SNR comparison across all experiments (baseline, AWGN, AWGN_v2, curriculum, and SE+Reverb) is shown below:
![Accuracyâ€“SNR Curve](runs/acc_snr.png)

### ğŸ“˜ Visualizing Noisy / Curriculum / SE-Reverb Spectrograms

This project also includes an auxiliary script for visualizing how different processing methods
(noise corruption, curriculum training stages, and SE-Reverb) affect the time-frequency structure
of speech signals.

This script generates three side-by-side spectrograms:

Noisy speech (0 dB AWGN)

Curriculum-style corrupted speech (â€“5 dB AWGN)

SE + Reverb processed speech

These plots help explain the qualitative differences mentioned in the report
(e.g., transient consonant degradation, smoothing effects, reverberation smearing).

â–¶ How to Run

From the project root directory, run:
```bash
python plot_spectrogram_variants.py
```
### ğŸ“ Output

The script will save the figure to:

```bash
figures/spectrogram_noisy_curriculum_sereverb.png
```

ğŸ“Œ Example Output
![Spectrogram Variants](figures/melspec_variants_noisy_curriculum_sereverb.png)
